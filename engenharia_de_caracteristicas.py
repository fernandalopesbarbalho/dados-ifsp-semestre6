# -*- coding: utf-8 -*-
"""Aula 100. Engenharia de Características.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11BSRxioEHhc2VcW3vLK75S7eSR3jQsMR

# Instalação de bibliotecas
"""

!pip install -U scikit-learn

"""# Importação de bibliotecas"""

import pandas as pd

# Importando a Técnica PCA e Método de normalização (Z-score)
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from numpy import set_printoptions

from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from numpy import set_printoptions

from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import mutual_info_classif
from numpy import set_printoptions

from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import RFE

from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectFromModel

from google.colab import drive

# Algoritmo de indução para verificar o desempenho dos subconjuntos de atributos
from sklearn.ensemble import RandomForestClassifier

"""# Montar o Google Drive"""

drive.mount('/content/drive', force_remount=True)

"""# Ler o arquivo

Conjunto de dados de [diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database) para descrição das técnicas de engenharia de características. O conjunto de dados consiste em vários atributos preditivos médicos (número de gestações que a paciente possuiu, IMC, nível de insulina, idade, entre outros) e um atributo alvo (tem ou não tem diabetes?).
"""

file_path = '/content/drive/MyDrive/Colab Notebooks/diabetes.csv'

df = pd.read_csv(file_path);

"""# Agregação de Atributos

## Análise de componentes principais (PCA, principal component analysis)

Utilize a técnica PCA com a biblioteca scikit-learn para redução dos atributos para 2 dimensões do *dataframe* **Diabetes**

- A redução deverá ocorrer apenas nos atributos preditivos
- A distribuição dos dados deverá ser transformada para normal antes de aplicar o PCA (consultar aulas anteriores)
- Utilize a propriedade `explained_variance_ratio_` do método **PCA** para explicar   quanta informação (variação) pode ser atribuída a cada um dos componentes principais.
"""

# @title Construa se código

X = df.drop('Outcome', axis=1)
y = df['Outcome']

# Normalização
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

print("Variância explicada por cada componente:", pca.explained_variance_ratio_)
print("Variância total explicada pelos 2 componentes:", sum(pca.explained_variance_ratio_))

df_pca = pd.DataFrame(data = X_pca, columns = ['Componente Principal 1', 'Componente Principal 2'])
df_pca_final = pd.concat([df_pca, y], axis = 1)

print("\nDataframe com os Componentes Principais:")
print(df_pca_final.head())

"""# Seleção de Atributos

## Seleção por Ordenação

### Método de seleção: Análise de Variância (ANOVA) -> F_classif

- A redução deverá ocorrer apenas nos atributos preditivos
- Selecione os 3 melhores atributos por meio do método de ordenação: **Análise de Variância (ANOVA)** -> `F_classif`
- Antes de aplicar o método ANOVA, normalize os dados por escala (0, 1) utilizando a função `MinMaxScaler()`
- Mostre a classificação dos atributos de acordo com o método **ANOVA**
- Mostre o nome das colunas selecionadas pelo método **ANOVA**
- Mostre o *dataframe* final criado a partir dos atributos selecionados
"""

# @title Construa seu código

X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_cols = X.columns

# Normalização
scaler = MinMaxScaler(feature_range=(0, 1))
X_rescaled = scaler.fit_transform(X)

# Seleciona os 3 melhores atributos com ANOVA
test = SelectKBest(score_func=f_classif, k=3)
fit = test.fit(X_rescaled, y)

print("Classificação dos atributos (scores ANOVA):")
set_printoptions(precision=3)
print(fit.scores_)

selected_indices = fit.get_support(indices=True)

selected_columns = X_cols[selected_indices]
print("\nColunas selecionadas pelo método ANOVA:", selected_columns.tolist())

df_anova = df[selected_columns]
print("\nDataframe final com os atributos selecionados:")
print(df_anova.head())

"""### Método de seleção: Qui-quadrado (Chi-Squared) -> chi2

- A redução deverá ocorrer apenas nos atributos preditivos
- Selecione os 3 melhores atributos por meio do método de ordenação: **Qui-quadrado** -> `chi2`
- Antes de aplicar o método Qui-quadrado, normalize os dados por escala (0, 1) utilizando a função `MinMaxScaler()`
- Mostre a classificação dos atributos de acordo com o método **Qui-quadrado**
- Mostre o nome das colunas selecionadas pelo método **Qui-quadrado**
- Mostre o *dataframe* final criado a partir dos atributos selecionados
"""

# @title Construa seu código

X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_cols = X.columns

# Normalização
scaler = MinMaxScaler(feature_range=(0, 1))
X_rescaled = scaler.fit_transform(X)

# Seleciona os 3 melhores atributos com Qui-quadrado
test = SelectKBest(score_func=chi2, k=3)
fit = test.fit(X_rescaled, y)

print("Classificação dos atributos (scores Qui-quadrado):")
set_printoptions(precision=3)
print(fit.scores_)

selected_indices = fit.get_support(indices=True)

selected_columns = X_cols[selected_indices]
print("\nColunas selecionadas pelo método Qui-quadrado:", selected_columns.tolist())

df_chi2 = df[selected_columns]
print("\nDataframe final com os atributos selecionados:")
print(df_chi2.head())

"""## Seleção por Complementaridade

### Método de seleção FILTRO

- A redução deverá ocorrer apenas nos atributos preditivos
- Selecione os 3 melhores atributos por meio do método de Complementariedade: **Informação Mútua** -> `mutual_info_classif`
- Antes de aplicar o método Informação Mútua, normalize os dados por escala (0, 1) utilizando a função `MinMaxScaler()`
- Mostre a classificação dos atributos de acordo com o método **Informação Mútua**
- Mostre o nome das colunas selecionadas pelo método **Informação Mútua**
- Mostre o *dataframe* final criado a partir dos atributos selecionados
"""

# @title Construa seu código

X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_cols = X.columns

# Normalização
scaler = MinMaxScaler(feature_range=(0, 1))
X_rescaled = scaler.fit_transform(X)

# Seleciona os 3 melhores atributos com Informação Mútua
test = SelectKBest(score_func=mutual_info_classif, k=3)
fit = test.fit(X_rescaled, y)

print("Classificação dos atributos (scores Informação Mútua):")
set_printoptions(precision=3)
print(fit.scores_)

selected_indices = fit.get_support(indices=True)

selected_columns = X_cols[selected_indices]
print("\nColunas selecionadas pelo método de Informação Mútua:", selected_columns.tolist())

df_mutual_info = df[selected_columns]
print("\nDataframe final com os atributos selecionados:")
print(df_mutual_info.head())

"""### Método de seleção ***WRAPPER***

Eliminação Recursiva de Atributos (Recursive Feature Elimination) utilizando modelo preditivo

- Selecione os 4 melhores atributos por meio do método de Complementariedade: **Recursive Feature Elimination (RFE)***
- Antes de aplicar o método RFE, normalize os dados por escala (0, 1) utilizando a função `MinMaxScaler()`
- Utilize o Algoritmo `RandomForestClassifier` de indução para verificar o desempenho dos subconjuntos de atributos
- Mostre o nome das colunas selecionadas pelo método **Informação Mútua**
- Mostre o *dataframe* final criado a partir dos atributos selecionados
"""

# @title Construa seu código

X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_cols = X.columns

# Normalização
scaler = MinMaxScaler(feature_range=(0, 1))
X_rescaled = scaler.fit_transform(X)

# Utiliza o RandomForestClassifier como modelo para o RFE
model = RandomForestClassifier(n_estimators=100, random_state=42)

rfe = RFE(model, n_features_to_select=4)
fit = rfe.fit(X_rescaled, y)

selected_columns = X_cols[fit.support_]
print("Colunas selecionadas pelo método RFE:", selected_columns.tolist())

df_rfe = df[selected_columns]
print("\nDataframe final com os atributos selecionados:")
print(df_rfe.head())

"""### Método de seleção ***EMBEDDED***

Seleção dos K-melhores atributos com base na importância dos atributos para um modelo

- Selecione os K-melhores atributos por meio do método de Complementariedade: **EMBEDDED***
- Antes de aplicar o método RFE, normalize os dados por escala (0, 1) utilizando a função `MinMaxScaler()`
- Utilize o Algoritmo `RandomForestClassifier` de seleção baseada na importância calculada de cada característica
- Valor limiar `0.1` de importância para seleção dos atributos
- Mostre a quantidade de colunas selecionadas
- Mostre o nome das colunas selecionadas pelo método **EMBEDDED**
- Mostre o *dataframe* final criado a partir dos atributos selecionados
"""

# @title Construa seu código

X = df.drop('Outcome', axis=1)
y = df['Outcome']
X_cols = X.columns

# Normalização
scaler = MinMaxScaler(feature_range=(0, 1))
X_rescaled = scaler.fit_transform(X)

# Utiliza o RandomForestClassifier para seleção baseada na importância
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_rescaled, y)

selector = SelectFromModel(model, prefit=True, threshold=0.1)

selected_columns_mask = selector.get_support()
selected_columns = X_cols[selected_columns_mask]

print("Quantidade de colunas selecionadas:", len(selected_columns))
print("Colunas selecionadas pelo método EMBEDDED:", selected_columns.tolist())

df_embedded = df[selected_columns]
print("\nDataframe final com os atributos selecionados:")
print(df_embedded.head())